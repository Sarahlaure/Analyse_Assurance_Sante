[["index.html", "Prévision des coûts médicaux ", " Prévision des coûts médicaux /* 1. RESET & FOND GLOBAL */ .book .book-body, .book .book-body .page-wrapper .page-inner { background-color: #f8fafc !important; /* Gris très clair \"Premium\" */ } /* On cache le titre automatique généré par Bookdown */ h1 { display: none; } /* 2. LOGOS */ .header-logos { display: flex; justify-content: space-between; align-items: center; margin: 20px auto 40px auto; max-width: 900px; } .logo-item img { height: 70px; filter: drop-shadow(0 4px 6px rgba(0,0,0,0.05)); transition: transform 0.3s ease; } .logo-item img:hover { transform: translateY(-5px); } /* 3. CARTE PRINCIPALE */ .main-cover-card { background: #ffffff; border-radius: 32px; padding: 60px 50px; box-shadow: 0 20px 50px rgba(0,0,0,0.04), 0 1px 3px rgba(0,0,0,0.02); border: 1px solid rgba(226, 232, 240, 0.8); text-align: center; max-width: 900px; margin: 0 auto 50px auto; position: relative; } .badge-pill { display: inline-block; padding: 8px 10px; background: linear-gradient(135deg, #eef2ff 0%, #e0e7ff 100%); color: #4338ca; border-radius: 99px; font-size: 0.75em; font-weight: 800; letter-spacing: 1px; margin-bottom: 10px; border: 1px solid #c7d2fe; } .main-title { display: block !important; font-size: 3.2em !important; font-weight: 900 !important; color: #0f172a !important; letter-spacing: -1.5px !important; line-height: 1.1 !important; margin-bottom: 15px !important; border: none !important; } .sub-title { font-size: 1.2em; color: #64748b; font-weight: 400; margin-bottom: 40px; } /* 4. GRILLE DES INFOS */ .cover-grid { display: flex; gap: 24px; text-align: left; } .cover-box { flex: 1; background: #fdfdfd; padding: 30px; border-radius: 24px; border: 1px solid #f1f5f9; box-shadow: inset 0 2px 4px rgba(0,0,0,0.01); } .cover-kicker { font-size: 0.65em; text-transform: uppercase; letter-spacing: 1.5px; color: #2D7FF9; font-weight: 800; margin-bottom: 15px; } .cover-names { font-size: 0.95em; font-weight: 600; color: #1e293b; line-height: 1.7; } .supervisor-name { font-size: 1.1em; font-weight: 800; color: #0f172a; } .cover-note { font-size: 0.8em; color: #94a3b8; margin-top: 10px; display: block; } .context-tag { display: inline-block; margin-top: 20px; padding: 10px 15px; background: #f8fafc; border-radius: 12px; border-left: 4px solid #2D7FF9; font-size: 0.8em; color: #475569; } Agence Nationale de la Statistique et de la Démographie (ANSD) École Nationale de la Statistique et de l’Analyse Économique (ENSAE) PRÉSENTATION DU PROJET – ANNÉE ACADÉMIQUE 2025-2026 Prévision des coûts médicaux Machine Learning I — Rapport d’analyse Rédigé par DIALLO Cheick Oumar FALL Ndeye Ramatoulaye Ndoye FOGWOUNG DJOUFACK Sarah-Laure NGUEMFOUO NGOUMTSA Célina RASAMOELINA Nihaviana Albert Paulinah Élèves ingénieurs statisticiens économistes (ISE 2) Sous la supervision de Mme DIAW Mously Freelance Senior Data Scientist / ML Engineer Projet réalisé dans le cadre du cours de machine learning. Document généré via R Bookdown utilisant les algorithmes CatBoost, XGBoost et LightGBM. "],["introduction.html", "1 Introduction 1.1 Contexte général 1.2 Contexte de l’étude et source des données 1.3 Objectifs du projet 1.4 Organisation du document", " 1 Introduction 1.1 Contexte général Depuis la seconde moitié du XXᵉ siècle, les dépenses de santé ont connu une croissance soutenue dans la quasi-totalité des pays développés. Cette évolution s’explique par plusieurs facteurs structurels : le progrès médical, le vieillissement de la population, l’augmentation de la prévalence des maladies chroniques et l’élargissement de la couverture assurantielle. Selon l’Organisation de Coopération et de Développement Économiques (OCDE), les dépenses de santé représentent aujourd’hui plus de 9 % du PIB en moyenne dans les pays membres, avec des perspectives de hausse à long terme. Au-delà de leur niveau moyen, la forte concentration des dépenses médicales individuelles demeure un fait stylisé majeur. Une minorité d’individus génère une part disproportionnée des coûts totaux, ce qui se traduit par une distribution fortement asymétrique à droite, marquée par la présence de valeurs extrêmes. Cette propriété statistique pose des défis importants en matière de modélisation, de prévision et d’évaluation des risques, notamment pour les assureurs et les décideurs publics. Dans ce contexte, le recours aux méthodes de machine learning apparaît comme un levier pertinent pour mieux comprendre les déterminants des dépenses de santé et améliorer la précision des prédictions individuelles. 1.2 Contexte de l’étude et source des données L’étude s’appuie sur la base de données Insurance, mise à disposition sur la plateforme Kaggle. Ce jeu de données synthétise des informations individuelles issues du système d’assurance santé américain. Il contient, pour chaque assuré, des caractéristiques démographiques (âge, sexe), comportementales (statut de fumeur), familiales (nombre d’enfants à charge), géographiques (région de résidence) ainsi que le montant annuel des charges médicales prises en charge par l’assurance. La variable d’intérêt principale, charges, correspond au coût médical annuel exprimé en dollars. La problématique centrale du projet peut être formulée comme suit : Comment prédire les charges médicales individuelles à partir de caractéristiques observables ? L’objectif de ce projet est donc de tirer parti de ces données afin de construire, comparer et évaluer plusieurs modèles de régression capables de prédire efficacement les charges médicales individuelles. 1.3 Objectifs du projet Les objectifs poursuivis dans ce travail sont multiples : Explorer et comprendre la structure des données, à travers une analyse descriptive approfondie des variables numériques et catégorielles. Mettre en place un prétraitement, incluant l’encodage des variables catégorielles, la gestion de l’asymétrie de la cible et le choix des méthodes de standardisation. Comparer plusieurs familles de modèles de régression, en cohérence avec le cours de machine learning : modèles linéaires (régression linéaire, Ridge, Lasso, Elastic Net), modèles non linéaires (KNN, arbre de décision, Random Forest, SVR), méthodes ensemblistes (bagging, boosting, stacking, voting). Évaluer les performances à l’aide de métriques communes (MAE, MSE, RMSE, R², MAPE), en combinant validation croisée et évaluation sur un jeu de test. Identifier le modèle le plus performant et analyser ses erreurs afin de mieux comprendre ses limites et ses points forts. 1.4 Organisation du document Le rapport est structuré de la manière suivante : Chapitre I — Analyse de la base de données : présentation des données, prétraitement, analyse descriptive et choix des variables explicatives et de la cible. Chapitre II — Entraînement des modèles : mise en œuvre des modèles linéaires, non linéaires et ensemblistes, avec optimisation des hyperparamètres par validation croisée. Chapitre III — Évaluation et comparaison des modèles : analyse comparative des performances, choix du modèle final et diagnostics d’erreurs. Conclusion : synthèse des résultats, limites du travail et pistes d’amélioration. Annexes : rappels théoriques sur les métriques d’évaluation et détails techniques complémentaires. "],["analyse-de-la-base-de-données.html", "2 Analyse de la base de données 2.1 Présentation des données 2.2 Prétraitement des données 2.3 Analyse descriptive 2.4 Transformation de la variable cible 2.5 Choix des variables explicatives et de la cible", " 2 Analyse de la base de données 2.1 Présentation des données Le jeu de données utilisé dans ce projet provient de la base Insurance disponible sur la plateforme Kaggle. Il contient des informations individuelles relatives à des assurés du système de santé américain, ainsi que le montant annuel des charges médicales associées. La base comporte 1 338 observations et 7 variables explicatives, accompagnées d’une variable cible représentant les charges médicales annuelles exprimées en dollars. 2.1.1 Description des variables Les variables disponibles sont les suivantes : age : âge de l’assuré (variable numérique continue). sex : sexe de l’assuré (male, female). bmi : indice de masse corporelle (Body Mass Index). children : nombre d’enfants à charge. smoker : statut tabagique (yes, no). region : région de résidence (northeast, northwest, southeast, southwest). charges : montant annuel des charges médicales (variable cible). Les variables explicatives comprennent à la fois des variables numériques et catégorielles, ce qui nécessite un prétraitement adapté avant toute modélisation. 2.2 Prétraitement des données 2.2.1 Valeurs manquantes et doublons Une vérification systématique de la présence de valeurs manquantes a été réalisée. Aucune valeur manquante n’a été détectée dans la base. Un doublon exact a toutefois été identifié. En l’absence d’information permettant de déterminer s’il s’agit d’une répétition accidentelle ou de deux individus distincts partageant exactement les mêmes caractéristiques observées, ce doublon a été supprimé par précaution. Ce choix vise à éviter une surreprésentation d’un profil particulier dans l’échantillon, susceptible d’introduire un biais dans l’estimation des modèles. 2.2.2 Typage des variables Les variables ont été correctement typées : les variables numériques (age, bmi, children, charges) sont traitées comme quantitatives ; les variables sex, smoker et region sont traitées comme catégorielles. Cette distinction est essentielle pour la mise en place des étapes d’encodage et de standardisation ultérieures. 2.3 Analyse descriptive 2.3.1 Analyse univariée des variables numériques Les variables numériques ont été analysées à l’aide de statistiques descriptives (moyenne, médiane, quartiles, écart-type) et de représentations graphiques (histogrammes et boxplots), afin d’évaluer leur distribution, leur dispersion et la présence éventuelle de valeurs extrêmes. Cette analyse met en évidence : une distribution relativement régulière pour les variables age et children, caractérisée par une dispersion modérée et une absence de valeurs extrêmes marquées ; une variabilité plus prononcée pour bmi, avec une dispersion intermédiaire mais sans asymétrie excessive (skweness=0,284) ; une forte asymétrie à droite pour la variable charges (skweness=1.514), avec une concentration des observations à des niveaux de coûts relativement faibles et la présence de valeurs extrêmes élevées. 2.3.2 Analyse univariée des variables catégorielles Les variables catégorielles ont été étudiées à partir des effectifs et des proportions associées à chacune de leurs modalités. Les résultats indiquent : une répartition globalement équilibrée entre les sexes (49,5% d’hommes et 50,5% de femmes) ; une majorité d’individus non-fumeurs, les fumeurs représentant une part plus restreinte de l’échantillon (20,49% de fumeurs contre 79,51% de non-fumeurs) ; une distribution relativement homogène des individus entre les différentes régions géographiques (24,23% ; 24,23% ; 24,31% et 27,23% pour respectivement le northeast, northwest, southwest et southeast). 2.3.3 Analyse bivariée avec la variable cible Une analyse bivariée a été menée afin d’étudier la relation entre chaque variable explicative et la variable cible charges, dans le but d’identifier les facteurs les plus discriminants et d’examiner la structure des coûts médicaux. 2.3.3.1 Variables numériques et charges Les relations entre charges et les variables numériques (age et bmi) ont été analysées à l’aide de nuages de points. Ces visualisations montrent : une relation globalement croissante entre age et charges, indiquant que les coûts médicaux tendent à augmenter avec l’âge, bien que la dispersion reste importante ; une relation positive plus marquée entre bmi et charges, avec une augmentation de la variabilité des charges pour des niveaux élevés de BMI. Ces éléments indiquent que les relations entre les variables numériques et la cible ne sont pas strictement linéaires et peuvent bénéficier de modèles capables de capturer des effets non linéaires. 2.3.3.2 Variables catégorielles et charges Les relations entre charges et les variables catégorielles (sex, smoker, region, children) ont été étudiées à l’aide de boxplots comparatifs. Les résultats montrent que : la variable smoker est fortement discriminante : les fumeurs présentent des niveaux de charges nettement plus élevés que les non-fumeurs, avec une médiane et une dispersion significativement supérieures ; pour les variables sex et region, les distributions des charges sont relativement proches entre les modalités, avec des médianes comparables et une dispersion similaire ; la variable children ne met pas en évidence de séparation nette des charges entre les différents nombres d’enfants à charge. 2.3.4 Test statistique : charges et statut tabagique Afin de confirmer cette observation, un test de Mann–Whitney a été réalisé pour comparer les distributions des charges entre fumeurs et non-fumeurs. Le résultat (p-value = 5.27e−130) conduit à rejeter l’hypothèse nulle d’égalité des distributions : les charges diffèrent significativement suivant que la personne soit fumeur ou non. Le test conduit à rejeter l’hypothèse d’égalité des distributions, indiquant une différence statistiquement significative des charges selon le statut tabagique. Cette variable apparaît ainsi comme un déterminant central des dépenses médicales. 2.4 Transformation de la variable cible L’analyse de la distribution de la variable charges révèle une asymétrie marquée à droite, confirmée par un coefficient d’asymétrie élevé (skweness=1.514). Afin de stabiliser la variance et de réduire l’influence des valeurs extrêmes, une transformation logarithmique de la cible a été envisagée : \\[ y^* = \\ln(1 + \\text{charges}) \\] Cette transformation permet d’obtenir une distribution nettement plus symétrique (skweness=-0.0897), comme le montrent les histogrammes et boxplots comparatifs entre charges et log(charges). Deux pistes de modélisation sont ainsi définies : une modélisation directe sur charges (interprétation immédiate en dollars) ; une modélisation sur log(1 + charges), suivie d’une retransformation par \\(\\exp(y^*) - 1\\) pour l’évaluation finale. 2.5 Choix des variables explicatives et de la cible L’ensemble des variables explicatives disponibles a été conservé pour la modélisation, compte tenu de leur pertinence économique et statistique. La variable cible retenue est : - charges pour la modélisation directe ; - log(1 + charges) pour la modélisation avec transformation de la cible. Ce cadre permet une comparaison rigoureuse des performances des modèles tout en respectant les contraintes méthodologiques liées à la distribution des données. Cette phase d’analyse et de prétraitement constitue le socle sur lequel repose l’entraînement et l’évaluation des modèles de régression présentés dans la suite du document. "],["modèles-de-régression-linéaire.html", "3 Modèles de régression linéaire 3.1 Environnement et bibliothèques utilisées 3.2 Préliminaires 3.3 Régression linéaire multiple (OLS) 3.4 Diagnostics statistiques des modèles linéaires 3.5 Tous les VIF sont compris entre 1 et 5. Il n’existe aucun niveau critique de multicolinéarité ; chaque variable apporte une information indépendante au modèle. 3.6 Régression Ridge 3.7 Le modèle Ridge sur cible directe est le plus performant en termes de variance expliquée (\\(R^2\\)), tandis que la version log réduit l’erreur relative (MAPE). 3.8 Régression Lasso 3.9 Régression Elastic Net 3.10 Sur la cible directe, l’ElasticNet converge vers un comportement Lasso (l1_ratio=1.0). Sur la cible log, il adopte un profil plus équilibré avec un ratio de 0.1. 3.11 Synthèse des résultats des modèles linéaires 3.12 Conclusion intermédiaire", " 3 Modèles de régression linéaire Cette section documente de manière exhaustive l’ensemble des modèles de régression linéaire implémentés dans le cadre de la prédiction des dépenses médicales individuelles (charges). Ces modèles constituent la première famille de méthodes étudiée. 3.1 Environnement et bibliothèques utilisées Les modèles linéaires ont été implémentés en Python à l’aide des bibliothèques suivantes : scikit-learn Utilisée pour : l’implémentation des modèles de régression linéaire (OLS, Ridge, Lasso, ElasticNet) ; la gestion des pipelines (Pipeline, ColumnTransformer) ; la validation croisée et la sélection d’hyperparamètres (GridSearchCV, KFold). numpy Utilisée pour : les transformations mathématiques (transformation logarithmique, retransformation exponentielle) ; le calcul de métriques personnalisées. pandas Utilisée pour : la manipulation des jeux de données ; la structuration et l’exportation des tableaux de résultats. statsmodels Utilisée pour : les diagnostics statistiques des modèles linéaires ; le calcul du Variance Inflation Factor (VIF). 3.2 Préliminaires 3.2.1 Découpage des données Le jeu de données a été scindé en deux sous-ensembles disjoints : 80 % des observations pour l’entraînement ; 20 % des observations pour le test. Ce découpage est effectué une seule fois, puis conservé strictement identique pour l’ensemble des modèles afin de garantir une comparaison équitable. Aucune information issue de l’échantillon de test n’est utilisée : - lors du prétraitement, - lors de la sélection des hyperparamètres, - lors de la validation croisée. 3.2.2 Prétraitement des variables explicatives Les variables explicatives sont de nature mixte : numériques : age, bmi, children ; catégorielles : sex, smoker, region. Un prétraitement systématique est intégré dans des pipelines scikit-learn, assurant une application cohérente et reproductible. 3.2.2.1 Encodage des variables catégorielles Les variables catégorielles sont transformées par One-Hot Encoding, avec : suppression d’une modalité de référence (drop = \"first\") afin d’éviter la colinéarité parfaite ; gestion explicite des modalités inconnues (handle_unknown = \"ignore\"). 3.2.2.2 Traitement des variables numériques Les variables numériques sont : - soit conservées telles quelles, - soit mises à l’échelle selon la méthode testée. Toutes les transformations sont apprises exclusivement sur l’échantillon d’entraînement, puis appliquées à l’échantillon de test. 3.2.3 Mise à l’échelle des variables numériques Bien que les modèles linéaires ne nécessitent pas systématiquement de standardisation, plusieurs méthodes de mise à l’échelle ont été testées afin d’évaluer leur impact sur : la stabilité numérique des estimations ; l’efficacité des pénalisations de type Ridge et Lasso ; les performances prédictives. Les méthodes évaluées sont : StandardScaler ; MinMaxScaler ; MaxAbsScaler ; absence de mise à l’échelle (passthrough). Le choix optimal est effectué par validation croisée, conjointement à la sélection des hyperparamètres. 3.2.4 Transformation de la variable cible L’analyse exploratoire a mis en évidence une forte asymétrie à droite de la variable charges. Deux cadres de modélisation ont donc été étudiés : Modélisation directe de charges Modélisation de log(1 + charges) Dans le second cas : - les modèles sont entraînés sur la cible transformée ; - les prédictions sont retranscrites sur l’échelle originale via : \\[ \\widehat{charges} = \\exp(\\widehat{y}^*) - 1 \\] Toutes les métriques sont calculées en dollars, afin de garantir une comparaison. 3.2.5 Métriques d’évaluation Les performances sont évaluées à l’aide des métriques suivantes : RMSE : pénalise fortement les grandes erreurs ; MSE : version quadratique brute de la RMSE ; MAE : erreur absolue moyenne, plus robuste aux valeurs extrêmes ; R² : proportion de la variance expliquée ; MAPE : erreur relative moyenne en pourcentage. Les métriques sont calculées : - sur l’ensemble d’entraînement ; - sur l’ensemble de test. Cette double évaluation permet de détecter un éventuel surapprentissage. 3.3 Régression linéaire multiple (OLS) La régression linéaire classique constitue le modèle de référence du projet. Elle repose sur les hypothèses suivantes : - relation linéaire entre les variables explicatives et la cible ; - erreurs centrées et indépendantes. Ce modèle ne comporte aucun hyperparamètre et sert de baseline pour l’évaluation des modèles régularisés. 3.4 Diagnostics statistiques des modèles linéaires Au-delà des performances prédictives, plusieurs diagnostics statistiques ont été réalisés : Moyenne des résidus La moyenne des résidus indique si le modèle a tendance à surestimer ou sous-estimer globalement la cible. Approche Directe (Charges) : La moyenne est de \\(1.4981833441919948 \\times 10^{-11}\\). Approche Logarithmique (Log) : La moyenne est de \\(4.488722200384253 \\times 10^{-15}\\). Dans les deux cas, la valeur est quasi nulle, ce qui confirme que nos modèles sont globalement non biaisés sur l’échantillon d’entraînement. Normalité des résidus, afin d’évaluer la validité des hypothèses du modèle ; Le test d’Anderson-Darling vérifie si les erreurs suivent une loi normale. Une p-value &lt; 5% entraîne le rejet de la normalité. Cible p-value AD Conclusion Charges Directes 0.0 Hypothèse \\(H_0\\) rejetée Log Charges 0.0 Hypothèse \\(H_0\\) rejetée Pour les charges directes, les résidus ne suivent pas une distribution normale. La transformation logarithmique, bien qu’elle réduise l’asymétrie de la cible, ne permet pas de corriger la non-normalité des résidus pour ce modèle. Homoscedasticité, pour vérifier la constance de la variance des erreurs ; Le test de White vérifie la constance de la variance des résidus. * Modèle Charges : La p-value est de \\(1.0679 \\times 10^{-14}\\). On ne peut pas accepter l’hypothèse nulle d’homoscédasticité au risque 5%. * Modèle Log : La p-value est de \\(3.6375 \\times 10^{-16}\\). L’hypothèse nulle est également rejetée. Analyse Comparée : L’hétéroscédasticité est présente dans les deux modèles. Cela signifie que l’incertitude de nos prédictions varie en fonction des caractéristiques des individus (ex: les erreurs peuvent être plus grandes pour les fumeurs que pour les non-fumeurs). Multicolinéarité, mesurée par le Variance Inflation Factor (VIF). Le VIF est calculé uniquement sur les variables numériques, après transformation, afin d’éviter les effets artificiels liés aux variables indicatrices issues du One-Hot Encoding. Variable VIF État age 1.014954 Corrélation quasi nulle bmi 1.014891 Corrélation quasi nulle children 1.000239 Corrélation quasi nulle 3.5 Tous les VIF sont compris entre 1 et 5. Il n’existe aucun niveau critique de multicolinéarité ; chaque variable apporte une information indépendante au modèle. 3.6 Régression Ridge La régression Ridge introduit une pénalisation de type \\(\\ell_2\\), permettant : de réduire la variance du modèle ; de stabiliser les coefficients ; d’atténuer les effets de la multicolinéarité. Le paramètre de régularisation \\(\\alpha\\) est sélectionné par validation croisée à 5 plis, conjointement au choix de la mise à l’échelle. Métrique Cible Directe (charges) Cible Transformée (log) Meilleur Scaling MaxAbsScaler() MaxAbsScaler() Alpha Optimal 0.222299 0.449843 R² Test 0.73336 0.480835 RMSE Test 6151.99 8584.37 MAPE Test (%) 42.61 % 28.92 % 3.7 Le modèle Ridge sur cible directe est le plus performant en termes de variance expliquée (\\(R^2\\)), tandis que la version log réduit l’erreur relative (MAPE). 3.8 Régression Lasso La régression Lasso repose sur une pénalisation \\(\\ell_1\\), permettant : la régularisation du modèle ; la sélection automatique de variables via l’annulation de certains coefficients. Le protocole expérimental est identique à celui de la régression Ridge. Métrique Cible Directe (charges) Cible Transformée (log) Meilleur Scaling passthrough (Aucun) passthrough (Aucun) Alpha Optimal 79.0604 0.001 R² Test 0.73246 0.47346 RMSE Test 6162.33 8645.12 MAPE Test (%) 43.29 % 28.89 % Lasso préfère l’absence de mise à l’échelle (passthrough) pour les deux approches. Ses performances sont très proches de Ridge, indiquant peu de variables inutiles dans le dataset. 3.9 Régression Elastic Net La régression Elastic Net combine les pénalisations Ridge et Lasso via deux hyperparamètres : \\(\\alpha\\) : intensité globale de la régularisation ; \\(l_1\\_ratio\\) : compromis entre \\(\\ell_1\\) et \\(\\ell_2\\). Une recherche sur grille est effectuée par validation croisée afin d’identifier la combinaison optimale. Métrique Cible Directe (charges) Cible Transformée (log) Meilleur Scaling StandardScaler() MinMaxScaler() Alpha Optimal 10.0 0.001 L1_ratio Optimal 1.0 0.1 R² Test 0.73299 0.48504 MAPE Test (%) 42.91 % 28.95 % 3.10 Sur la cible directe, l’ElasticNet converge vers un comportement Lasso (l1_ratio=1.0). Sur la cible log, il adopte un profil plus équilibré avec un ratio de 0.1. 3.11 Synthèse des résultats des modèles linéaires Modèle MAE Test MAPE (%) RMSE Test \\(R^2_{Test}\\) Ridge 4237.15 42.61 6152.00 0.7334 Régression linéaire 4243.54 42.82 6154.05 0.7332 ElasticNet 4249.78 42.91 6156.25 0.7330 Lasso 4278.63 43.29 6162.33 0.7325 ElasticNet (log) 4455.17 28.96 8549.47 0.4850 Ridge (log) 4465.52 28.93 8584.37 0.4808 Lasso (log) 4481.68 28.89 8645.13 0.4735 Régression linéaire (log) 4486.07 28.83 8654.32 0.4723 Les modèles linéaires entraînés sur la cible non transformée présentent les meilleures performances globales en termes de RMSE et de \\(R^2\\), avec des résultats très proches entre OLS, Ridge, Lasso et ElasticNet. Les modèles entraînés sur log(1 + charges) affichent : - une dégradation du pouvoir explicatif global ; - mais une amélioration notable de l’erreur relative (MAPE). 3.12 Conclusion intermédiaire Les modèles linéaires atteignent un plafond de performance, suggérant que les relations entre les variables explicatives et les charges médicales comportent des non-linéarités et des interactions complexes. Ces constats justifient le recours à des modèles non linéaires et ensemblistes, présentés dans la section suivante. "],["régressions-non-linéaires.html", "4 Régressions non linéaires 4.1 Environnement et bibliothèques utilisées 4.2 Rappels 4.3 K-Nearest Neighbors (KNN Regressor) 4.4 Bien que le \\(R^2\\) soit plus élevé sur les charges directes, l’approche Log est plus pertinente objectivement car elle réduit l’erreur relative (MAPE) de 35.35 % à 24.25 %. La transformation log permet de lisser l’influence des voisins aux coûts extrêmes. 4.5 Arbre de décision (DecisionTreeRegressor) 4.6 Random Forest (RandomForestRegressor) 4.7 Support Vector Regression (SVR) 4.8 Tableau comparatif final (modèles non linéaires)", " 4 Régressions non linéaires Cette sous-partie présente les modèles non linéaires entraînés après le prétraitement des données. 4.1 Environnement et bibliothèques utilisées Les modèles non linéaires ont été implémentés en Python principalement via : scikit-learn modèles : KNeighborsRegressor, DecisionTreeRegressor, RandomForestRegressor, SVR pipelines : Pipeline, ColumnTransformer prétraitement : OneHotEncoder, scalers (StandardScaler, MinMaxScaler, RobustScaler, etc.) validation croisée et optimisation : GridSearchCV, KFold métriques : RMSE (via score négatif), MAE, R² numpy transformation de la cible (log1p, expm1) calculs auxiliaires (MAPE, agrégation des scores) pandas structuration des résultats export CSV des tableaux finaux Remarque : le choix de GridSearchCV combiné à des pipelines garantit que toutes les transformations (encodage + scaling) sont apprises uniquement sur le train dans chaque fold de validation croisée, évitant ainsi tout data leakage. 4.2 Rappels 4.2.1 Découpage train / test Le dataset est découpé une seule fois en : Train : 80 % Test : 20 % Ce split est conservé identique pour tous les modèles afin de rendre les comparaisons fiables. 4.2.2 Prétraitement et pipeline anti-fuite Les variables explicatives comprennent : numériques : age, bmi, children catégorielles : sex, smoker, region Le prétraitement est intégré dans un ColumnTransformer : catégorielles : OneHotEncoder(drop=\"first\", handle_unknown=\"ignore\") numériques : scaling testé selon les modèles Important : scaling uniquement sur les numériques Les colonnes issues du OneHotEncoding sont des indicatrices (0/1). Les “scaler” doivent être appliqués uniquement aux variables numériques, afin de : - conserver le sens des indicatrices ; - éviter une transformation inutile qui peut dégrader les performances. 4.2.3 Validation croisée et optimisation Validation croisée : 5 folds (cv = 5) Optimisation : recherche sur grille avec GridSearchCV Critère CV : neg_root_mean_squared_error Ce score est négatif par convention scikit-learn : maximiser neg_RMSE revient à minimiser RMSE. Une fois les meilleurs hyperparamètres sélectionnés en CV, le modèle est : refité sur tout le train (refit=True) ; évalué une seule fois sur le test. 4.2.4 Deux stratégies de modélisation de la cible Comme pour les modèles linéaires, deux cadres sont comparés : Apprentissage direct sur charges Apprentissage sur log(1 + charges) Dans le second cas, la prédiction se fait en deux étapes : prédiction sur l’échelle log : \\(\\widehat{y}^*\\) retransformation en dollars : \\[ \\widehat{charges} = \\exp(\\widehat{y}^*) - 1 \\] Toutes les métriques reportées restent en dollars, afin de pouvoir comparer tous les modèles sur la même échelle. Métriques reportées (train et test) : MSE, RMSE, MAE, R², MAPE. 4.3 K-Nearest Neighbors (KNN Regressor) Le modèle KNN repose sur une logique de similarité : pour prédire la charge d’un individu, on considère ses k voisins les plus proches dans l’espace des variables explicatives. 4.3.1 Pourquoi le scaling est indispensable ici ? KNN utilise une distance (euclidienne, Manhattan, etc.). Si les variables numériques ne sont pas à la même échelle (ex. age vs bmi), la distance est dominée par les variables à grande amplitude. Ainsi, le choix du scaling influence directement la performance. ### Hyperparamètres optimisés Les paramètres optimisés en validation croisée incluent typiquement : n_neighbors : nombre de voisins \\(k\\) weights : \"uniform\" ou \"distance\" éventuellement p : distance Manhattan (1) vs Euclidienne (2) scaler : méthode de scaling appliquée aux variables numériques uniquement 4.3.2 Sorties retenues meilleure combinaison (hyperparamètres + scaling), métriques Train/Test, RMSE test comme indicateur principal de comparaison. Métrique / Config Cible Directe (Charges) Cible Transformée (Log) Meilleur Scaling MinMaxScaler() MinMaxScaler() Voisins (k) 5 10 Poids (Weights) distance uniform R² Test 0.7549 0.7201 RMSE Test 5898.21 6302.61 MAPE Test (%) 35.35 % 24.25 % Approche Directe : Le modèle affiche un excellent \\(R^2\\) de 0.7549, mais présente un surapprentissage massif sur l’entraînement avec un \\(R^2_{train}\\) de 0.9983, dû au poids distance. Approche Log : Le modèle est plus équilibré avec un \\(R^2_{train}\\) de 0.7798 et offre une meilleure généralisation sur le test. 4.4 Bien que le \\(R^2\\) soit plus élevé sur les charges directes, l’approche Log est plus pertinente objectivement car elle réduit l’erreur relative (MAPE) de 35.35 % à 24.25 %. La transformation log permet de lisser l’influence des voisins aux coûts extrêmes. 4.5 Arbre de décision (DecisionTreeRegressor) Le modèle Decision Tree construit des règles successives de partition de l’espace des données afin de prédire charges. 4.5.0.1 Particularité (scaling généralement inutile) Les arbres sont basés sur des seuils et des réductions d’impureté : ils ne dépendent pas d’une distance globale. La mise à l’échelle n’est donc pas nécessaire. 4.5.0.2 Hyperparamètres optimisés (GridSearchCV) max_depth : profondeur maximale de l’arbre (contrôle la complexité). min_samples_leaf : taille minimale d’une feuille (régularisation). Métrique / Config Cible Directe (Charges) Cible Transformée (Log) Meilleur Scaling MinMaxScaler() StandardScaler() Max Depth 5 5 Min Samples Leaf 10 20 R² Test 0.8257 0.8325 RMSE Test 4973.29 4875.52 MAPE Test (%) 29.89 % 20.80 % Approche Directe : L’arbre franchit un cap de performance majeur avec un \\(R^2\\) de 0.8257. Approche Log : Les performances s’améliorent encore avec une RMSE plus basse de 4875.52. Le modèle Log l’emporte sur tous les indicateurs. La MAPE est réduite de 9 points (20.80 % contre 29.89 %), confirmant que la structure hiérarchique de l’arbre est plus efficace pour prédire des ordres de grandeur que des valeurs monétaires brutes asymétriques. 4.6 Random Forest (RandomForestRegressor) La Random Forest est une méthode ensembliste basée sur le bagging : - entraînement de plusieurs arbres sur des sous-échantillons bootstrap, - agrégation (moyenne) des prédictions. - Elle réduit la variance d’un arbre unique et produit en général une performance plus stable. 4.6.1 Hyperparamètres optimisés (GridSearchCV) n_estimators : nombre d’arbres. max_depth : profondeur maximale. min_samples_leaf : taille minimale d’une feuille. Métrique / Config Cible Directe (Charges) Cible Transformée (Log) Meilleur Scaling StandardScaler() RobustScaler() N Estimators 100 (Baseline) 200 Min Samples Leaf 20 10 R² Test 0.8325 0.8350 RMSE Test 4875.52 4839.95 MAPE Test (%) 20.80 % 18.28 % Approche Directe : Offre une performance robuste, identique à l’arbre de décision log en termes de \\(R^2\\) (0.8325). Approche Log : Atteint le sommet des performances avec un \\(R^2\\) de 0.8350 et une MAPE minimale de 18.28 %. Le Random Forest (log) est le meilleur modele non linéaire dans le cadre de cette étude. L’utilisation du RobustScaler combinée à la transformation log permet de gérer efficacement les individus ayant des coûts atypiques, offrant la prédiction la plus précise et stable du projet. 4.7 Support Vector Regression (SVR) La SVR apprend une fonction de prédiction en maximisant une marge et en tolérant une bande d’erreur contrôlée (epsilon-insensitive loss). 4.7.1 Pourquoi le scaling est indispensable ici ? SVR repose sur des distances dans l’espace transformé (kernel RBF le plus souvent). Sans scaling, le modèle devient instable et les hyperparamètres perdent leur sens. 4.7.1.1 Hyperparamètres optimisés (GridSearchCV) C : pénalisation des erreurs gamma : influence locale des points (kernel) epsilon : zone de tolérance autour de la prédiction kernel : linéaire / RBF (selon votre grille) scaler numérique : testé conjointement Métrique / Config Cible Directe (Charges) Cible Transformée (Log) Meilleur Scaling passthrough passthrough Kernel / C linéaire / 100 linéaire / 100 R² Test 0.5178 0.5178 RMSE Test 8273.47 8273.47 MAPE Test (%) 20.58 % 20.58 % Analyse Individuelle &amp; Comparée : Les résultats sont strictement identiques entre les deux versions avec un noyau linéaire (R² = 0.5178). Objectivement, le SVR est moins performant que les arbres ou le KNN sur ce dataset. Bien que sa MAPE de 20.58 % soit compétitive, son faible pouvoir explicatif global (\\(R^2\\)) limite son utilité pratique par rapport à une forêt aléatoire. 4.8 Tableau comparatif final (modèles non linéaires) Les résultats sont synthétisés sous forme d’un tableau unique, comportant : - les métriques sur train (diagnostic d’apprentissage), - les métriques sur test (généralisation), - les meilleurs hyperparamètres issus de la validation croisée. library(readxl) library(dplyr) library(DT) # 1. Lecture du fichier res_models &lt;- read.csv(&quot;metrics_lin_non_lin.xls&quot;) # 2. Nettoyage res_display &lt;- res_models %&gt;% select(-any_of(c(&quot;MSE_train&quot;, &quot;MSE_test&quot;))) %&gt;% mutate(across(where(is.numeric), ~ round(., 2))) %&gt;% arrange(RMSE_test) # 5. Rendu Dynamique datatable(res_display, rownames = FALSE, caption = &quot;Scénario 1 : charges + OneHotEncoder (Pipeline)&quot;, options = list(scrollX = TRUE, pageLength = 6)) Le tableau de résultats présente les performances de plusieurs modèles de régression évalués sur les ensembles d’entraînement et de test à l’aide de métriques standard : \\(R^2\\), RMSE, MSE, MAE et MAPE. 4.8.1 Performances globales sur l’ensemble de test Les meilleures performances globales sur l’ensemble de test sont obtenues par les modèles arborescents. Le Random Forest (log) atteint le meilleur \\(R^2_{test}\\) (0,835) ainsi que le RMSE Test le plus faible (4 839,95). Il présente également la MAPE Test la plus basse (18,28 %) parmi l’ensemble des modèles évalués. Les modèles Decision Tree (log) et Random Forest (sur charges) affichent des performances très proches, avec un \\(R^2_{test}\\) de 0,833 et un RMSE Test de 4 875,52. Leur MAPE Test est identique (20,80 %). Le Decision Tree linéaire obtient un \\(R^2_{test}\\) légèrement inférieur (0,826) et un RMSE Test plus élevé (4 973,29), accompagné d’une MAPE Test nettement plus importante (29,89 %). Les modèles linéaires pénalisés (Ridge, ElasticNet, Lasso) présentent des performances inférieures : - Les valeurs de \\(R^2_{test}\\) sont comprises entre 0,732 et 0,733. - Les RMSE Test dépassent 6 150. - Les MAPE Test sont élevées, comprises entre 42,6 % et 43,3 %. Les SVR linéaires, avec ou sans transformation logarithmique, affichent un \\(R^2_{test}\\) de 0,518, un RMSE Test supérieur à 8 270, et une MAPE Test d’environ 20,58 %. Les versions logarithmiques des modèles linéaires pénalisés (Ridge log, Lasso log, ElasticNet log) obtiennent les plus faibles \\(R^2_{test}\\), compris entre 0,47 et 0,49, avec des RMSE Test supérieurs à 8 500 et des MAPE Test proches de 29 %. 4.8.2 Erreur relative (MAPE) L’analyse de la MAPE Test met en évidence des écarts importants : Les MAPE Test les plus faibles sont observées pour : Random Forest (log) : 18,28 % Decision Tree (log) et Random Forest : 20,80 % SVR et SVR (log) : 20,58 % Les MAPE les plus élevées concernent les modèles linéaires pénalisés entraînés sur la cible linéaire, avec des valeurs supérieures à 42 %. Les modèles linéaires pénalisés entraînés sur la cible logarithmique affichent des MAPE Test intermédiaires, proches de 29 %. 4.8.3 Comparaison entraînement / test L’écart entre les performances d’entraînement et de test permet d’évaluer la généralisation des modèles : Les modèles Random Forest et Decision Tree présentent des \\(R^2_{train}\\) compris entre 0,877 et 0,887, contre 0,826 à 0,835 en test, indiquant un écart modéré. Les modèles linéaires pénalisés affichent des performances proches entre Train et Test, avec des \\(R^2\\) autour de 0,75, tant en entraînement qu’en test. Les SVR montrent des valeurs de \\(R^2\\) quasi identiques entre Train et Test (≈ 0,538), accompagnées d’erreurs élevées sur les deux ensembles. Les modèles entraînés sur la cible logarithmique présentent systématiquement des MAPE Train inférieures aux MAPE Test, quel que soit le type de modèle. 4.8.4 Synthèse Les modèles arborescents, en particulier le Random Forest (log), obtiennent les meilleures performances globales sur l’ensemble de test selon les métriques \\(R^2\\), RMSE et MAPE. Les modèles linéaires pénalisés présentent des performances plus faibles, tant en termes d’erreur absolue que relative. Les modèles SVR affichent une capacité explicative limitée malgré des erreurs relatives modérées. La transformation logarithmique de la variable cible est associée à une réduction de la MAPE pour les modèles arborescents, mais pas pour les modèles linéaires pénalisés. L’analyse des résultats montre que les modèles non linéaires, et plus particulièrement la Random Forest, améliorent significativement les performances prédictives par rapport aux modèles linéaires initialement étudiés. Ces résultats motivent l’étude approfondie des méthodes ensemblistes avancées, qui visent à optimiser encore davantage la précision par un apprentissage séquentiel. La section suivante sera consacrée à l’implémentation et à la comparaison des algorithmes de boosting les plus performants "],["méthodes-ensemblistes.html", "5 Méthodes ensemblistes 5.1 Environnement et bibliothèques utilisées 5.2 Processus expérimental commun (anti-fuite) 5.3 Description des algorithmes utilisés 5.4 Resultats des 4 scénarios évalués 5.5 Comparaison des résultats", " 5 Méthodes ensemblistes Après l’évaluation des modèles linéaires et non linéaires, nous avons étendu l’analyse aux méthodes ensemblistes. Ces approches combinent plusieurs modèles (souvent des arbres de décision) afin d’améliorer la généralisation, réduire la variance (bagging) ou le biais (boosting), et mieux capturer les relations non linéaires présentes dans les dépenses médicales (charges). L’objectif de cette section est de documenter fidèlement : les imports / bibliothèques et leur rôle, le processus complet (split → preprocessing → CV → test), les 4 scénarios testés (cible + gestion des catégorielles), les hyperparamètres optimisés et la méthode de sélection, les meilleurs paramètres obtenus (best params) et les tableaux de synthèse. 5.1 Environnement et bibliothèques utilisées Les méthodes ensemblistes ont été implémentées en Python avec : scikit-learn Pipelines et prétraitement : Pipeline, ColumnTransformer, OneHotEncoder Validation croisée et tuning : KFold, GridSearchCV Modèles : RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor xgboost XGBRegressor (Gradient Boosting optimisé, régularisation, efficacité) lightgbm LGBMRegressor (boosting très rapide via stratégie leaf-wise) catboost CatBoostRegressor (optimisé pour variables catégorielles, robuste au surapprentissage) numpy / pandas Transformations (log1p, expm1), structuration des résultats, export CSV Note : l’ensemble des modèles et de la CV utilisent la même graine (random_state) pour rendre les comparaisons cohérentes. 5.2 Processus expérimental commun (anti-fuite) 5.2.1 Split train/test Découpage unique : 80 % train / 20 % test Le test reste totalement isolé : aucun prétraitement appris sur test, aucune sélection d’hyperparamètres faite sur test, le test sert uniquement à l’évaluation finale. 5.2.2 Prétraitement (Pipeline) Deux stratégies de gestion des variables catégorielles ont été comparées : Encodage One-Hot (OHE) via ColumnTransformer Traitement natif des catégorielles (LightGBM / CatBoost), sans OHE 5.2.2.1 1) Cas avec OneHotEncoder (scénarios 1 et 2) catégorielles → OneHotEncoder(drop=\"first\", handle_unknown=\"ignore\") numériques → passthrough Ce prétraitement est intégré dans un pipeline : Pipeline(preprocess → model) Ainsi, à chaque fold de CV : - le prétraitement est fit uniquement sur le sous-train, - puis appliqué au sous-validation, ce qui élimine le data leakage. 5.2.2.2 2) Cas natif (scénarios 3 et 4) Les colonnes catégorielles sont converties en type category Les modèles LightGBM et CatBoost apprennent directement avec ces variables Objectif : tester si l’OHE est nécessaire ou si le traitement natif est plus performant/stable 5.2.3 Validation croisée + optimisation CV : KFold(n_splits = 5, shuffle = TRUE, random_state = ...) Optimisation : GridSearchCV Critère de sélection : neg_root_mean_squared_error (équivalent à minimiser la RMSE) Chaque modèle est donc entraîné selon la logique : pipeline construit, recherche sur grille effectuée uniquement sur train, meilleur modèle refité sur tout le train, métriques calculées sur train et test. 5.2.4 Métriques reportées Pour chaque modèle, les métriques sont calculées sur : Train : diagnostic d’apprentissage / surapprentissage Test : performance de généralisation Métriques utilisées : MSE, RMSE (critère principal), MAE, R², MAPE(%). Les tableaux finaux incluent également Best_Params (hyperparamètres retenus par CV). 5.3 Description des algorithmes utilisés L’analyse s’appuie sur deux familles de méthodes ensemblistes : le Bagging et le Boosting. 5.3.1 Le Bagging (Bootstrap Aggregating) Random Forest : Construit plusieurs arbres de décision indépendants sur des sous-ensembles aléatoires de données. La prédiction finale est la moyenne des prédictions de tous les arbres. Intérêt : Réduit fortement la variance et le risque de surapprentissage (overfitting). 5.3.2 Le Boosting (Apprentissage séquentiel) Contrairement au Bagging, les modèles de Boosting construisent des arbres les uns après les autres. Chaque nouvel arbre tente de corriger les erreurs commises par les précédents. AdaBoost : Ajuste les poids des observations : les individus mal prédits reçoivent un poids plus important pour le prochain arbre. Gradient Boosting : Optimise une fonction de perte en ajoutant des arbres qui prédisent les résidus (erreurs) du modèle précédent. XGBoost : Une version optimisée du Gradient Boosting incluant une régularisation avancée et une gestion efficace de la mémoire. LightGBM : Utilise une croissance des arbres par “feuilles” (leaf-wise) plutôt que par niveaux, ce qui le rend extrêmement rapide sur de grands jeux de données. CatBoost : Algorithme spécialisé dans le traitement des variables catégorielles. Il réduit le besoin de prétraitement (comme le One-Hot Encoding) et limite les biais de prédiction. Famille Modèle Principal Atout Bagging Random Forest Robustesse et stabilité face au bruit. Boosting AdaBoost Simplicité et focus sur les cas difficiles. Boosting Gradient Boosting Grande flexibilité et précision élevée. Boosting XGBoost Performance pure et régularisation (évite le surapprentissage). Boosting LightGBM Vitesse d’exécution et faible consommation mémoire. Boosting CatBoost Gestion native et performante des variables qualitatives. 5.4 Resultats des 4 scénarios évalués Nous avons séparé les expérimentations selon deux axes : Transformation de la cible : charges log1p(charges) + retransformation expm1() Traitement des catégorielles : via OneHotEncoder (pipeline scikit-learn) via traitement natif (LightGBM / CatBoost) Cela conduit aux quatre scénarios suivants. 5.4.1 Scénario 1 — charges + OneHotEncoder (Pipeline) cible : charges X : variables catégorielles encodées en OHE + numériques passthrough sélection d’hyperparamètres : GridSearchCV sur train (CV=5) métriques : calculées sur train et sur test (échelle dollars) Modèles évalués : RandomForest, GradientBoosting, AdaBoost, XGBoost, LightGBM, CatBoost. 5.4.2 Scénario 2 — log1p(charges) + OneHotEncoder (Pipeline) cible : charges X : identique au scénario 1 (OHE + numériques) après prédiction : retransformation expm1() pour revenir aux dollars métriques : calculées en dollars (comparées à charges réelles) Ce scénario permet de tester si une cible plus “stable” statistiquement améliore la performance finale en dollars. Modèles évalués : RandomForest, GradientBoosting, AdaBoost, XGBoost, LightGBM, CatBoost. 5.4.3 Scénario 3 — charges + traitement natif des catégorielles (sans OHE) modèles concernés : LightGBM et CatBoost X : colonnes catégorielles converties en type category cible : charges sélection d’hyperparamètres : GridSearchCV sur train (CV=5) métriques : train/test (dollars) 5.4.4 Scénario 4 — log1p(charges) + traitement natif (sans OHE) modèles concernés : LightGBM et CatBoost X : colonnes catégorielles converties en type category cible : log1p(charges) retransformation expm1() avant calcul des métriques en dollars sélection d’hyperparamètres : GridSearchCV sur train (CV=5) métriques : train/test (dollars) 5.5 Comparaison des résultats L’analyse des métriques de performance met en évidence plusieurs conclusions. 5.5.1 Performance globale des modèles Le modèle CatBoost (avec One-Hot Encoding ou traitement natif des variables catégorielles) présente les meilleures performances globales, avec le **RMSE Test le plus faible (4 721,29 \\()** et le **meilleur coefficient de détermination** (\\)R^2_{Test} = 0,8429$). Ces résultats indiquent une capacité supérieure à limiter les erreurs de grande amplitude sur la variable charges. 5.5.2 Supériorité des méthodes de Gradient Boosting Les algorithmes de Gradient Boosting (GradientBoosting, XGBoost, CatBoost) surpassent systématiquement le Random Forest et AdaBoost en termes de précision quadratique (RMSE). Cette performance s’explique par leur mécanisme d’apprentissage itératif, qui corrige progressivement les erreurs résiduelles et permet une meilleure capture des relations non linéaires complexes. 5.5.3 Optimisation de l’erreur relative (MAPE) Une divergence marquée est observée selon la transformation de la variable cible. Les modèles entraînés sur \\(\\log(1 + charges)\\) (notamment XGBoost Log et LightGBM Log) affichent des MAPE Test significativement plus faibles, comprises entre 17 % et 18 %, contre environ 30 % pour les modèles entraînés sur la cible linéaire. L’apprentissage sur l’échelle logarithmique favorise la minimisation de l’erreur relative plutôt que de l’erreur absolue. Ainsi, bien que le RMSE soit légèrement moins performant, ces modèles produisent des prédictions plus justes en pourcentage, en particulier pour les petites et moyennes factures médicales. 5.5.4 Conclusion En conclusion, CatBoost Standard apparaît comme le modèle le plus performant pour minimiser les erreurs absolues et maximiser la qualité globale des prédictions. Néanmoins, lorsque l’objectif est d’assurer une meilleure précision relative, notamment pour les faibles montants, les modèles entraînés sur une cible logarithmique constituent une alternative particulièrement pertinente. "],["selection-modele.html", "6 Sélection et Validation du Modèle Final 6.1 Synthèse Comparative des Performances 6.2 Discussion Technique et Justification du Choix 6.3 Sélection du Modèle", " 6 Sélection et Validation du Modèle Final L’évaluation systématique des différentes architectures de régression, allant des modèles linéaires régularisés aux méthodes d’ensemble avancées, permet d’identifier la solution optimale pour la prédiction des charges d’assurance. Cette sélection repose sur un compromis entre le pouvoir explicatif (\\(R^2\\)), la minimisation de l’erreur quadratique (\\(RMSE\\)) et la précision relative (\\(MAPE\\)). 6.1 Synthèse Comparative des Performances L’analyse des résultats consolidés démontre une progression nette de la performance à mesure que la complexité des modèles augmente, soulignant la nature non linéaire des données. Catégorie Modèle Cible \\(R^2\\) Test \\(RMSE\\) \\(MAPE\\) (%) Ensemble (Boosting) CatBoost Directe 0.8429 4721.29 30.97 Ensemble (Bagging) Random Forest Log 0.8350 4839.95 18.28 Non-Linéaire Decision Tree Log 0.8325 4875.52 20.80 Linéaire (Base) Ridge Directe 0.7334 6151.99 42.61 6.2 Discussion Technique et Justification du Choix L’examen des métriques révèle deux modèles dominants selon l’objectif métier privilégié : 6.2.1 1. Optimisation de la Variance Globale : CatBoost Le modèle CatBoost, appliqué sur la cible directe, présente le coefficient de détermination le plus élevé de l’étude (\\(R^2 = 0.8429\\)). * Rigueur Statistique : Il minimise la \\(RMSE\\) (4721.29), ce qui indique une meilleure gestion des valeurs extrêmes (outliers) présentes dans les charges médicales élevées. * Architecture : Ses hyperparamètres optimaux (depth: 4, iterations: 200, learning_rate: 0.03) démontrent une complexité maîtrisée évitant le surapprentissage. 6.2.2 2. Optimisation de l’Erreur Relative : Random Forest (Log) Le modèle Random Forest utilisant la transformation logarithmique de la cible se distingue par la plus faible Erreur Absolue Moyenne en Pourcentage (\\(MAPE = 18.28\\%\\)). * Analyse de la Transformation : La transformation \\(y&#39; = \\ln(y)\\) stabilise la variance et réduit l’impact de l’asymétrie à droite de la distribution des charges, permettant une précision relative supérieure sur les petits et moyens budgets. 6.3 Sélection du Modèle Le modèle CatBoost (Cible Directe) est retenu comme le modèle final car: 1. Pouvoir Explicatif Maximal : Avec un \\(R^2\\) de \\(0.8429\\), ce modèle explique plus de 84 % de la variance des charges, surpassant la baseline linéaire de plus de 11 points. 2. Traitement Natif des Variables : CatBoost traite de manière optimale les variables catégorielles (sexe, fumeur, région), limitant les biais d’encodage. 3. Robustesse : La proximité entre les scores d’entraînement (\\(R^2 = 0.8825\\)) et de test (\\(R^2 = 0.8429\\)) témoigne d’une excellente capacité de généralisation du modèle. 6.3.1 Configuration Finale du Modèle Retenu Algorithme : CatBoostRegressor Prétraitement : One-Hot Encoding pour les variables catégorielles Hyperparamètres : depth : 4, iterations : 200, learning_rate : 0.03 Note de Conclusion : La supériorité des méthodes ensemblistes avancées confirme que la structure des coûts d’assurance est régie par des interactions complexes, notamment entre l’indice de masse corporelle (IMC) et le statut de fumeur, que seuls les modèles à base d’arbres régularisés parviennent à capturer pleinement. "],["conclusion-générale.html", "7 Conclusion générale 7.1 Résumé des résultats 7.2 Limites de l’étude 7.3 Recommandations et perspectives 7.4 Bibliographie", " 7 Conclusion générale 7.1 Résumé des résultats Ce projet avait pour objectif principal de prédire les dépenses médicales individuelles (charges) à partir de caractéristiques socio-démographiques et comportementales issues de la base Insurance (Kaggle). Au-delà de la simple recherche de performance prédictive, l’enjeu était de comparer rigoureusement plusieurs familles de modèles, tout en respectant des principes méthodologiques stricts : reproductibilité, absence de fuite d’information, validation croisée et analyse fine des métriques. L’étude a été structurée autour de trois grandes étapes : Analyse exploratoire et prétraitement des données (EDA) Cette phase a permis : d’identifier une forte asymétrie de la variable cible charges ; de mettre en évidence le rôle central du statut tabagique dans la variabilité des coûts médicaux ; de justifier l’exploration d’une transformation logarithmique de la cible. Modélisation linéaire et non linéaire Les modèles linéaires (OLS, Ridge, Lasso, ElasticNet) ont servi de baseline. Malgré leur interprétabilité et leur stabilité, ils atteignent rapidement un plafond de performance, suggérant l’existence de relations non linéaires et d’interactions complexes entre les variables explicatives. Les modèles non linéaires (KNN, SVR, arbres de décision) ont permis d’améliorer partiellement les performances, mais restent sensibles : au choix des hyperparamètres, à la mise à l’échelle, au surapprentissage pour certains modèles. Méthodes ensemblistes (Bagging et Boosting) Les méthodes ensemblistes constituent le cœur des résultats du projet. Les algorithmes de Gradient Boosting, en particulier CatBoost, XGBoost et LightGBM, obtiennent les meilleures performances globales, tant en termes de RMSE que de \\(R^2\\). Deux constats majeurs émergent : l’apprentissage sur la cible non transformée maximise la précision absolue (RMSE) ; l’apprentissage sur \\(\\log(1 + charges)\\) améliore significativement la MAPE, donc la précision relative, notamment pour les faibles et moyens niveaux de dépenses. En synthèse, le projet montre que : - les modèles linéaires sont insuffisants pour capturer toute la complexité des données ; - les modèles ensemblistes, et en particulier les méthodes de boosting, constituent les approches les plus adaptées à ce problème de prédiction. 7.2 Limites de l’étude Malgré les résultats obtenus, plusieurs limites doivent être soulignées. 7.2.1 Limites liées aux données Taille du jeu de données : La base Insurance contient environ 1 300 observations, ce qui reste modeste pour des modèles complexes comme le boosting. Cela peut accentuer le risque de surapprentissage. Nombre limité de variables explicatives : Les charges médicales dépendent de nombreux facteurs non observés ici (pathologies chroniques, antécédents médicaux, accès aux soins, couverture assurantielle détaillée, etc.). Contexte spécifique : Les données concernent le système de santé américain. Les résultats ne sont donc pas directement généralisables à d’autres contextes institutionnels ou géographiques. 7.2.2 Limites méthodologiques Absence de validation externe : Les performances sont évaluées uniquement via un découpage train/test sur une seule base. Aucune validation sur un jeu de données indépendant n’a été possible. Interprétabilité limitée des modèles ensemblistes : Bien que performants, les modèles de boosting sont plus difficiles à interpréter que les modèles linéaires, ce qui peut constituer un frein dans un cadre réglementaire ou décisionnel. 7.3 Recommandations et perspectives Plusieurs pistes d’amélioration et d’extension peuvent être envisagées. 7.3.1 Améliorations méthodologiques Validation croisée imbriquée (nested CV) Permettrait d’obtenir une estimation plus robuste des performances en séparant strictement la sélection des hyperparamètres de l’évaluation finale. Analyse d’interprétabilité avancée Importance des variables (feature importance), SHAP values ou méthodes similaires pour expliquer les prédictions des modèles de boosting. Optimisation plus fine des hyperparamètres L’usage de méthodes bayésiennes (ex. Bayesian Optimization, Optuna) pourrait améliorer la recherche d’hyperparamètres par rapport à une grille fixe. 7.3.2 Améliorations liées aux données Intégrer des variables médicales plus riches (pathologies, traitements, historique des soins). Ajouter des variables socio-économiques (revenu, type de contrat, couverture assurantielle). Tester le modèle sur des bases plus larges et plus hétérogènes. 7.3.3 Perspectives applicatives Utilisation du modèle pour : la segmentation des assurés, l’anticipation des coûts futurs, l’aide à la tarification ou à la gestion du risque. 7.4 Bibliographie Breiman, L. (2001). Random Forests. Machine Learning, 45(1), 5–32. Friedman, J. H. (2001). Greedy function approximation: A gradient boosting machine. Annals of Statistics, 29(5), 1189–1232. Friedman, J. H. (2002). Stochastic Gradient Boosting. Computational Statistics &amp; Data Analysis, 38(4), 367–378. Hastie, T., Tibshirani, R., &amp; Friedman, J. (2009). The Elements of Statistical Learning. Springer. Pedregosa, F. et al. (2011). Scikit-learn: Machine Learning in Python. Journal of Machine Learning Research, 12, 2825–2830. Chen, T., &amp; Guestrin, C. (2016). XGBoost: A Scalable Tree Boosting System. KDD. Ke, G. et al. (2017). LightGBM: A Highly Efficient Gradient Boosting Decision Tree. NeurIPS. Prokhorenkova, L. et al. (2018). CatBoost: unbiased boosting with categorical features. NeurIPS. Kaggle. Medical Cost Personal Datasets. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
