introduction
contexte-général
contexte-de-létude-et-source-des-données
objectifs-du-projet
organisation-du-document
analyse-de-la-base-de-données
présentation-des-données
description-des-variables
prétraitement-des-données
valeurs-manquantes-et-doublons
typage-des-variables
analyse-descriptive
analyse-univariée-des-variables-numériques
analyse-univariée-des-variables-catégorielles
analyse-bivariée-avec-la-variable-cible
variables-numériques-et-charges
variables-catégorielles-et-charges
test-statistique-charges-et-statut-tabagique
transformation-de-la-variable-cible
choix-des-variables-explicatives-et-de-la-cible
modèles-de-régression-linéaire
environnement-et-bibliothèques-utilisées
préliminaires
découpage-des-données
prétraitement-des-variables-explicatives
encodage-des-variables-catégorielles
traitement-des-variables-numériques
mise-à-léchelle-des-variables-numériques
transformation-de-la-variable-cible-1
métriques-dévaluation
régression-linéaire-multiple-ols
diagnostics-statistiques-des-modèles-linéaires
tous-les-vif-sont-compris-entre-1-et-5.-il-nexiste-aucun-niveau-critique-de-multicolinéarité-chaque-variable-apporte-une-information-indépendante-au-modèle.
régression-ridge
le-modèle-ridge-sur-cible-directe-est-le-plus-performant-en-termes-de-variance-expliquée-r2-tandis-que-la-version-log-réduit-lerreur-relative-mape.
régression-lasso
régression-elastic-net
sur-la-cible-directe-lelasticnet-converge-vers-un-comportement-lasso-l1_ratio1.0.-sur-la-cible-log-il-adopte-un-profil-plus-équilibré-avec-un-ratio-de-0.1.
synthèse-des-résultats-des-modèles-linéaires
conclusion-intermédiaire
régressions-non-linéaires
environnement-et-bibliothèques-utilisées-1
rappels
découpage-train-test
prétraitement-et-pipeline-anti-fuite
validation-croisée-et-optimisation
deux-stratégies-de-modélisation-de-la-cible
k-nearest-neighbors-knn-regressor
pourquoi-le-scaling-est-indispensable-ici
sorties-retenues
bien-que-le-r2-soit-plus-élevé-sur-les-charges-directes-lapproche-log-est-plus-pertinente-objectivement-car-elle-réduit-lerreur-relative-mape-de-35.35-à-24.25-.-la-transformation-log-permet-de-lisser-linfluence-des-voisins-aux-coûts-extrêmes.
arbre-de-décision-decisiontreeregressor
particularité-scaling-généralement-inutile
hyperparamètres-optimisés-gridsearchcv
random-forest-randomforestregressor
hyperparamètres-optimisés-gridsearchcv-1
support-vector-regression-svr
pourquoi-le-scaling-est-indispensable-ici-1
hyperparamètres-optimisés-gridsearchcv-2
tableau-comparatif-final-modèles-non-linéaires
performances-globales-sur-lensemble-de-test
erreur-relative-mape
comparaison-entraînement-test
synthèse
méthodes-ensemblistes
environnement-et-bibliothèques-utilisées-2
processus-expérimental-commun-anti-fuite
split-traintest
prétraitement-pipeline
cas-avec-onehotencoder-scénarios-1-et-2
cas-natif-scénarios-3-et-4
validation-croisée-optimisation
métriques-reportées
description-des-algorithmes-utilisés
le-bagging-bootstrap-aggregating
le-boosting-apprentissage-séquentiel
resultats-des-4-scénarios-évalués
scénario-1-charges-onehotencoder-pipeline
scénario-2-log1pcharges-onehotencoder-pipeline
scénario-3-charges-traitement-natif-des-catégorielles-sans-ohe
scénario-4-log1pcharges-traitement-natif-sans-ohe
comparaison-des-résultats
performance-globale-des-modèles
supériorité-des-méthodes-de-gradient-boosting
optimisation-de-lerreur-relative-mape
conclusion
selection-modele
synthèse-comparative-des-performances
discussion-technique-et-justification-du-choix
optimisation-de-la-variance-globale-catboost
optimisation-de-lerreur-relative-random-forest-log
sélection-du-modèle
configuration-finale-du-modèle-retenu
conclusion-générale
résumé-des-résultats
limites-de-létude
limites-liées-aux-données
limites-méthodologiques
recommandations-et-perspectives
améliorations-méthodologiques
améliorations-liées-aux-données
perspectives-applicatives
bibliographie
